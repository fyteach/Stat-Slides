---
title: "Topic 4: Linear Relationship"
author: "Fei Ye"
date: '`r format(Sys.time(), "%B %Y")`'
output:
  markdown::html_format:
    keep_md: true
    meta:
      css: [default, slides, "meta-data/custom.css", "@npm/@xiee/utils/css/key-buttons.min.css, heading-anchor.min.css"]
      js: [slides, "@npm/@xiee/utils/js/key-buttons.min.js, external-link.min.js, heading-anchor.min.js", "meta-data/lastupdated.js"]
      header-includes: "meta-data/Hypothesis-config.html"
    options:
      number_sections: true
      latex_math: true
      js_math:
        package: mathjax
        version: 3
        js: es5/tex-svg.js
---

```{r, echo=FALSE, warning=FALSE}
knitr::opts_chunk$set(comment="#", fig.retina=2, crayon.enabled = TRUE)
library(stats)
library(kableExtra)
library(knitr)
library(formattable)
library(ggplot2)
library(ggthemes)
library(ggExtra)
library(dplyr,warn.conflicts = FALSE)
library(latex2exp)
set.seed(923)
```

## Learning Goals

- Summarize and interpret the relationship between two quantitative variables.

- Demonstrate understanding of concepts pertaining to linear regression.

- Use regression equations to make predictions and understand its limits.

---

## Scatterplots

- Correlation refers to a relationship between two quantitative variables:  
  
  - the independent (or explanatory) variable, usually denoted by $x$.
  
  - the dependent (or response) variable, usually denoted by $y$.

- **Example:** In a study of education attainment and annual salary, the years of education is the explanatory variable and the annual salary is the response variable.

- To describe the relationship between two quantitative variables, statisticians use a scatterplot.

- In a scatterplot, we describe the overall pattern with descriptions of direction, form, and strength.

---

## Direction of Linear Relationship

:::: {.row}
::: {.pull-left}
- **Positive relationship**: the response variable (y) increases when the explanatory variable (x) increases.

```{r grade, echo=FALSE, result='asis', message=FALSE, fig.align = 'center', fig.align = 'center', fig.width=6, fig.asp=1}
 load("Data-Frames-SUNY-Concepts-in-Statistics/gradebook.rdata")
 gradebook <- rename(data)
 ggplot(head(gradebook,20), aes(x=Midterm1, y=Final)) +
   geom_point(size = 5) +
   geom_smooth(method=lm, se=FALSE) +
   theme_bw()
```
:::

::: {.pull-right}
- **Negative relationship**: the response variable (y) decreases when the explanatory variable (x) increases.
  
```{r mtcar, echo=FALSE, result='asis', message=FALSE, fig.align = 'center', fig.width=6, fig.asp=1}
ggplot(head(mtcars,20), aes(x=wt, y=mpg)) +
  geom_point(size = 5) +
  geom_smooth(method=lm, se=FALSE) +
  theme_bw()
```  
:::
::::

---

## Forms of Scatterplots

:::: {.row}
::: {style="width: 33%;"}

```{r cellphone, echo=FALSE, result='asis', message=FALSE, fig.align = 'center', fig.width=6, fig.asp=1}
load("Data-Frames-SUNY-Concepts-in-Statistics/cell_phones.RData")
cellphone <- rename(data)
ggplot(na.omit(head(cellphone,20)), aes(x=Math, y=Verbal)) +
  geom_point(size = 5) +
  geom_smooth(method=lm, se=FALSE) +
  theme_bw() +
  ggtitle("Linear Form") +
  theme(plot.title = element_text(size = 30, face = "bold", hjust = 0.5))
```  
:::

::: {style="width: 33%;"}

```{r Anscombe, echo=FALSE, result='asis', message=FALSE, fig.align = 'center', fig.width=6, fig.asp=1}
Anscombe.II.x<- c(10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0)
Anscombe.II.y<- c(9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74)
Anscombe.data <- data.frame(Anscombe.II.x, Anscombe.II.y)
ggplot(Anscombe.data, aes(x=Anscombe.II.x, y=Anscombe.II.y)) +
  geom_point(size = 5) +
  geom_smooth(method=loess, se=FALSE) +
  theme_bw() +
  ggtitle("Curvilinear Form") +
  theme(plot.title = element_text(size = 30, face = "bold", hjust = 0.5))
```
:::

::: {style="width: 33%;"}

```{r datAB, echo=FALSE, result='asis', message=FALSE, fig.align = 'center', fig.width=6, fig.asp=1}
set.seed(955)
dat <- data.frame(cond = rep(c("A", "B"), each=10),
                  xvar = 1:10 + rnorm(20,sd=2),
                  yvar = 1:10 + rnorm(10,sd=5))
ggplot(dat, aes(x=xvar, y=yvar)) +
  geom_point(size = 5) +
  geom_smooth(method=lm, se=FALSE) +
  theme_bw()  +
  xlab("x") +
  ylab("y") +
  ggtitle("No obvious relationship") + 
  theme(plot.title = element_text(size = 30, face = "bold", hjust = 0.5))
```
:::
::::

---

## Strength of Relationship

The strength of the relationship is a description of how closely the data follow the form of the relationship.

::::: {.row}
:::: {.pull-left}
```{r rstong, echo=FALSE, result='asis', message=FALSE, fig.align = 'center', fig.width=7, fig.asp=1}
x1  <- rnorm(n = 100, mean = 1, sd = 2)
v1  <- rnorm(n = 100, mean = 0, sd = 3)
m1 <- sd(v1)
err1   <- rnorm(n=100, mean=0, sd = 1)
y1   <- m1*(x1-mean(x1)) + err1
dat1   <- data.frame(x=x1, y=y1)
ggplot(dat1, aes(x=x1, y=y1)) +
  geom_point(size = 5) +
  geom_smooth(method=lm, se=FALSE) + 
  theme_bw() +
  xlab("x") +
  ylab("y") -> g1
cor1 <- format(round(cor(x1, y1, method="pearson"), 3), nsmall=3)
txt1 <- paste("Stronger Relation ($r=", cor1, "$)")
g1 + 
  ggtitle("Stronger Relation") + 
  # ggtitle(TeX(txt1)) + 
  theme(plot.title = element_text(size = 30, face = "bold", hjust = 0.5, vjust = 1))
```
::::

:::: {.pull-right}
```{r rweak, echo=FALSE, result='asis', message=FALSE, fig.align = 'center', fig.width=7, fig.asp=1}
x2  <- x1
v2  <- rnorm(n = 100, mean = 0, sd = 3)
m2 <- sd(v1)
err2   <- rnorm(n=100, mean=0, sd = 6)
y2   <- m2*(x2-mean(x2)) + err2
dat2   <- data.frame(x=x2, y=y2)
ggplot(dat1, aes(x=x2, y=y2)) +
  geom_point(size = 5) +
  geom_smooth(method=lm, se=FALSE) + 
  theme_bw() +
  xlab("x") +
  ylab("y") -> g2
cor2 <- format(round(cor(x2, y2, method="pearson"), 3), nsmall=3)
txt2 <- paste("Weaker Relation ($r=", cor2, "$)")
g2 +   
  ggtitle("Weaker Relation") + 
  # ggtitle(TeX(txt2)) + 
  theme(plot.title = element_text(size = 30, face = "bold", hjust = 0.5, vjust = 1))
```
::::
:::::

---

## Outliers

Outliers are points that deviate from the pattern of the relationship.

```{r outlier, echo=FALSE, result='asis', message=FALSE, fig.align = 'center', fig.width=7.5, fig.asp=1}
xx  <- rnorm(n = 100, mean = 0, sd = 1.5)
yy  <- (xx-mean(xx)) + rnorm(n = 100, mean = 0, sd = 0.2)
outlier_index <- sample(1:100, 1)
yy[outlier_index] <- yy[outlier_index] + 12

dat   <- data.frame(x=xx, y=yy)
ggplot(dat, aes(x=xx, y=yy)) +
  geom_point(size = 5) +
  geom_smooth(method=lm, se=FALSE) + 
  theme_bw() +
  xlab("x") +
  ylab("y") -> outlier
cor <- format(round(cor(xx, yy, method="pearson"), 3), nsmall=3)
# txt <- paste("$r=", cor, "$")
outlier + 
  ggtitle("Outlier Affects Regression") + 
  # ggtitle(TeX(txt)) + 
  theme(plot.title = element_text(size = 30, face = "bold", hjust = 0.5, vjust = 1))
```

---

## Practice: Match Scatterplots {.unnumbered}

:::: {.row style="font-size: .9em; margin-top:-1em;"}

::: {.pull-left}

![](Figures/MatchScatterplots.png){width="80%"}

**A:** X = month (January = 1), Y = rainfall (inches) in Napa, CA in 2010 (Note: Napa has rain in the winter months and months with little to no rainfall in summer.)

**B:** X = month (January = 1), Y = average temperature in Boston MA in 2010 (Note: Boston has cold winters and hot summers.)

:::

::: {.pull-right}

**C:** X = year (in five-year increments from 1970), Y = Medicare costs (in $) (Note: the yearly increase in Medicare costs has gotten bigger and bigger over time.)

**D:** X = average temperature in Boston MA (°F), Y = average temperature in Boston MA (°C) each month in 2010

**E:** X = chest girth (cm), Y = shoulder girth (cm) for a sample of men

**F:** X = engine displacement (liters), Y = city miles per gallon for a sample of cars (Note: engine displacement is roughly a measure of engine size. Large engines use more gas.)

:::

::::

::: {.footmark}
Source: [Scatterplots 2 of 5 in Concepts of Statistics](https://courses.lumenlearning.com/wmopen-concepts-statistics/chapter/scatterplots-2-of-5/)
:::

---

## The Correlation Coefficient

The **correlation coefficient** $r$ is a numeric measure that measures the strength and direction of a linear relationship between two quantitative variables. One definition is the mean product of standard values.
$$r=\dfrac{\sum\left(\frac{x-\bar{x}}{s_x}\right)\left(\frac{y-\bar{y}}{s_y}\right)}{\sqrt{\sum\left(\frac{x-\bar{x}}{s_x}\right)^2}\cdot\sqrt{\left(\frac{y-\bar{y}}{s_y}\right)^2}}=\dfrac{\sum\left(\frac{x-\bar{x}}{s_x}\right)\left(\frac{y-\bar{y}}{s_y}\right)}{n-1},$$
where $n$ is the sample size, $x$ is a data value for the explanatory variable, $\bar{x}$ is the mean of the $x$-values, $s_x$ is the standard deviation of the $x$-values, and similarly, for the notations involving $y$.

::: {.footmark}
See the paper [Thirteen Ways to Look at the Correlation Coefficient](https://www.stat.berkeley.edu/~rabbee/correlation.pdf) for other definitions.
:::

---

## A Few Remarks on Correlation Coefficient

- The expression $z=\dfrac{x-\bar{x}}{s_x}$ is known as the **standardized variable (or $z$-score)** which
  
  - doesn't depend on the unit of the variable $x$,
  
  - has mean $0$ and standard deviation 1.
  
- In Excel, the correlation coefficient can be calculated using the function `CORREL()`.

- [Scatterplots with different correlation coefficients](https://courses.lumenlearning.com/wmopen-concepts-statistics/chapter/linear-relationships-2-of-4/).

- **Rounding Rule:** Round to the nearest thousandth for $r$, $m$ and $b$.

---

## Geometric Intuition

:::: {.row}

::: {.pull-left}

```{r AnscombeLM, echo=FALSE, result='asis', message=FALSE, fig.align = 'center', fig.width=7.5, fig.asp=1}
Anscombe.I.x<- c(10.00,8.00,13.00,9.00,11.00,14.00,6.00,4.00,12.00,7.00,5.00)
Anscombe.I.y<- c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68)
Anscombe.data <- data.frame(Anscombe.I.x, Anscombe.I.y)
ggplot(Anscombe.data, aes(x=Anscombe.I.x, y=Anscombe.I.y)) +
  geom_point(size = 5) +
  geom_smooth(method=lm, se=FALSE) + 
  geom_vline(xintercept=mean(Anscombe.data$Anscombe.I.x))+
  geom_hline(yintercept=mean(Anscombe.data$Anscombe.I.y))+
  theme_bw() +
  xlab("x") +
  ylab("y") -> p1
r1 <- format(round(cor(Anscombe.data$Anscombe.I.x, Anscombe.data$Anscombe.I.y, method="pearson"), 3), nsmall=3)
text1 <- paste("$r=", r1, "$")
p1 +   ggtitle(TeX(text1)) + 
  theme(plot.title = element_text(size = 30, face = "bold", hjust = 0.5, vjust = 1))
```
:::

::: {.pull-right}

```{r datABLM, echo=FALSE, result='asis', message=FALSE, fig.align = 'center', fig.width=7.5, fig.asp=1}
set.seed(955)
dat <- data.frame(cond = rep(c("A", "B"), each=10),
                  xvar = 1:10 + rnorm(20,sd=2),
                  yvar = 1:10 + rnorm(10,sd=5))
p2 <- ggplot(dat, aes(x=xvar, y=yvar)) +
  geom_point(size = 5) +
  theme_bw()  +
  xlab("x") +
  ylab("y") + 
  geom_vline(xintercept=mean(dat$xvar))+
  geom_hline(yintercept=mean(dat$yvar))
r2 <- format(round(cor(dat$xvar, dat$yvar, method="pearson"), 3), nsmall=3)
text2 <- paste("$r = ", r2, "$")
p2  +  ggtitle(TeX(text2)) + 
  theme(plot.title = element_text(size = 30, face = "bold", hjust = 0.5, vjust = 1))
```
:::
::::

---

## Geometric Conclusion

For the scatterplots in the previous slides, we see that

- $r>0$ if all points $(x-\bar{x}, y-\bar{y})$ are in the 1st and the 3rd quadrants.

- $r<0$ if all points $(x-\bar{x}, y-\bar{y})$ are in the 2nd and the 4th quadrants.

- $r$ is bigger if points are closer to a line.

One idea of using product is from the geometric interpretation of $\mathbf{u}\cdot\mathbf{v}=\lVert \mathbf{u}\rVert\lVert\mathbf{v}\rVert\cos\theta$. There are also other interpretations of $r$.

---

## Properties

- The correlation coefficient $r$ is between $-1$ and $1$.

- The closer the absolute value $|r|$ is to $1$, the stronger the linear relationship is. Conventionally, the relationship is strong if $|r| > 0.8$, moderate if $0.5< |r|\le 0.8$, and weak if $|r|\le 0.5$.

- The correlation is symmetric in $x$ and $y$, that is `CORREL(x, y)=CORREL(y, x)`.

- The correlation does not change when the units of measurement of either one of the variables change. In other words, if we change the units of measurement of the explanatory variable and/or the response variable, it has no effect on the correlation  $r$.

::: {.footmark}
After discussing regression lines, you will see why $r^2\le 1$ and $r$ measures linear relationship or you may read [18.4 - More on Understanding Rho](https://online.stat.psu.edu/stat414/lesson/18/18.4#paragraph--775).
:::

---

## Limitations and Sensitivity to Outliers

- The correlation by itself is not enough to determine whether a relationship is linear. It's important to graph data set before analyzing it. [See Francis Anscombe's demonstration both the importance of graphing data and the effect of outliers on statistical properties.](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)

- The correlation is heavily influenced by outliers. [Try the simulation in Linear Relation (4 of 4) in Concepts in Statistics](https://courses.lumenlearning.com/wmopen-concepts-statistics/chapter/linear-relationships-4-of-4/)

---

## Practice: Guess the Correlation Coefficient {.unnumbered}

::: {.iframecontainer}
`r knitr::include_url("https://istats.shinyapps.io/guesscorr/", height="550px")`
:::

::: {.footmark}
Source: <https://istats.shinyapps.io/guesscorr/>
:::

---

## Example: The Correlation Coefficient (1 of 2)

:::: {.row style="font-size: 0.9em; margin-top=-2em;"}

::: {style="width: 68%;"}
Describe the relationship between Midterm 1 and Final for a sample of 10 students with data shown on the right.

**Solution:** First we create a scatterplot.

```{r cor_grade, echo=FALSE, result='asis', message=FALSE, fig.align = 'center', fig.width=4, fig.asp=1}
ggplot(gradebook[20:30,c("Midterm1", "Final")], aes(x=Midterm1, y=Final)) +
  geom_point(size = 5) +
  geom_smooth(method=lm, se=FALSE) +
  theme_bw()
```

Using the Excel function `CORREL(x, y)`, we find the correlation coefficient is
$r=`r format(round(cor(gradebook[20:30,c("Midterm1", "Final")]$Midterm1, gradebook[20:30,c("Midterm1", "Final")]$Final, method="pearson"), 3), nsmall=3)`$ .

The $r$-value shows a **strong positive linear** relationship.

:::

::: {style="width: 30%; margin-top=-1em;"}
```{r tab_grade, echo=FALSE, result='asis'}
kable(gradebook[20:30,c("Midterm1", "Final")],
    format = "html",
    booktabs = TRUE,
    align = "c",
   row.names = FALSE
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 24) %>%
  column_spec(1:2, width = "7em")
```
:::
::::

---

## Example: The Correlation Coefficient (2 of 2)

:::: {.row}
::: {style="width: 38%;"}

The correlation coefficient $r$ can also be calculation by hand using the formula.
$\dfrac{\sum z_xz_y}{n-1}$, where $z_x=\frac{x-\bar{x}}{s_x}$ and $z_y=\frac{y-\bar{y}}{s_y}$.
:::

::: {style="width:60%; font-size:0.7em;"}
| Midterm1 | Final    | z_x      | z_y      | z_xy        |
| -------- | -------- | -------- | -------- | ----------- |
| 72       | 72       | -0.78006 | -1.06926 | 0.834087814 |
| 93       | 88       | 1.50088  | 1.544483 | 2.318083715 |
| 81       | 82       | 0.197484 | 0.56433  | 0.111446332 |
| 82       | 82       | 0.306101 | 0.56433  | 0.172741815 |
| 94       | 88       | 1.609497 | 1.544483 | 2.485839773 |
| 80       | 77       | 0.088868 | -0.25246 | -0.02243591 |
| 73       | 78       | -0.67145 | -0.0891  | 0.059829084 |
| 71       | 77       | -0.88868 | -0.25246 | 0.224359064 |
| 81       | 76       | 0.197484 | -0.41582 | -0.08211835 |
| 81       | 76       | 0.197484 | -0.41582 | -0.08211835 |
| 63       | 68       | -1.75761 | -1.72269 | 3.027820885 |
| 79.18182 | 78.54545 |<- mean   | sum ->   | 9.047535876 |
| 9.206717 | 6.121497 |<- stdev.s|correl -> | 0.904753588 |
:::
::::

---

## Practice: Calculate Correlation Coefficient {.unnumbered}

::: {.iframecontainer}
`r knitr::include_url('https://www.myopenmath.com/embedq2.php?id=1716&seed=2020&showansafter')`
:::

---

## Correlation vs Causation

- Correlation is described by data from observational study. Observational studies cannot prove cause and effect which requires controlled study and rigorous inferences.

- Correlation may be used to make a prediction which is probabilistic.

- In a linear relationship, an $r$-value that is close to 1 or -1 is insufficient to claim that the explanatory variable causes changes in the response variable. The correct interpretation is that there is a statistical relationship between the variables.

- A **lurking variable** is a variable that is not measured in the study, but affects the interpretation of the relationship between the explanatory and response variables.

---

## Example: Correlation vs Causation (1 of 2)

The scatterplot below shows the relationship between the number of firefighters sent to fires (x) and the amount of damage caused by fires (y) in a certain city.
![](Figures/scatterplot-firefigters.png)

Can we conclude that the increase in firefighters causes the increase in damage?

::: {.footmark}
Source: [Causation and Lurking Variables in Concepts in Statistics for more example](https://courses.lumenlearning.com/wmopen-concepts-statistics/chapter/linear-relationships-4-of-4/)
:::

---

## Example: Correlation vs Causation (2 of 2)

**Solution:**

1. Correlation: The more fire fighters, the more likely there is bigger damage. However the fire fighters do not cause the fire.
  
2. Prediction: You could predict the amount of damage by looking at the number of fire fighters present.
  
3. Causation: The fire fighters are unlikely the cause of the fire.
  
4. Lurking variable: The seriousness of the fire is a lurking variable.

---

## Practice: Lurking Variable {.unnumbered}

::: {.iframecontainer}
`r knitr::include_url('https://www.myopenmath.com/embedq2.php?id=584887&seed=2020&showansafter')`
:::

---

## The Regression Line

- The line that best summarizes a linear relationship is **the least squares regression line**, that is, the line with the smallest sum of squares of the errors.
- For a value of the explanatory variable $x$, the corresponding $y$-values, denoted as $\hat{y}$, on the least-squares regression line can be used to predict the real $y$-value.
- The regression line is unique and passes though $(\bar{x}, \bar{y})$. An equation is given by
  $$\hat{y}=m(x-\bar{x})+\bar{y}=m x+b,$$
  where the slope is $m=\frac{\sum(x-\bar{x})(y-\bar{y})}{\sum(x-\bar{x})^2}=r\frac{s_y}{s_x}$ and the $y$-intercept is
  $b=\bar{y}-m\bar{x}.$
- The **residual** is $\text{Residual}=\text{Observed}-\text{Predicted}=y-\hat{y}.$
- A prediction beyond the range of the data is called **extrapolation**.

---

## Example: Old Faithful Geyser (1 of 2)

The following sample is taken from data about the Old Faithful geyser.

1. Study the linear relationship.
2. Find an equation of the regression line.
3. Find the predicated value and the residual when the eruption time is 1.8 minutes.

::::: {.row}
:::: {.pull-left}
::: {.center}
```{r tab_Geyser, echo=FALSE, result='asis'}
spring1 <- faithful[12:16, c("eruptions", "waiting")]
spring2 <- faithful[17:21, c("eruptions", "waiting")]
kable(cbind(spring1,spring2),
    format = "html", 
    booktabs = TRUE, 
    align = "c",
   row.names = FALSE
  ) %>%
  kable_styling(full_width = F, bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(1:4, width="6em")
```
:::
::::

::: {.pull-right}
```{r scatter_geyser, echo=FALSE, result='asis', message=FALSE, fig.align = 'center', fig.width=5, fig.asp=1}
spring <- faithful[12:21, c("eruptions", "waiting")]
ggplot(spring, aes(y=waiting, x=eruptions)) +
  geom_point(size = 5) +
  geom_smooth(method=lm, se=FALSE) +
  theme_bw()
```
:::
:::::

---

## Example: Old Faithful Geyser (2 of 2)

**Solution:** The Scatterplot shows a linear relationship.

`r reg.line=lm(spring$waiting ~ spring$eruptions)`

The slope of the regression line can be obtained using the Excel function `SLOPE()`. In this example, $m= `r coefx=round(reg.line$coefficients[2],3)` `r coefx`$.
  
The $y$-intercept $(0,b)$ can be obtained using the Excel function `INTERCEPT()`. In this example, $b= `r yint=round(reg.line$coefficients[1],3)` `r yint`$.
  
An equation of the line is then $\hat{y}=`r coefx`x + `r yint`$.

When $x=1.8$, we have $\hat{y}=`r coefx`*1.8 + `r yint`=`r ypred=coefx*1.8+yint` `r ypred`$.

The residual is $y-\hat{y}=`r spring[10, ]$waiting`-`r ypred`=`r error=spring[10,]$waiting-ypred` `r error`$. That means the predication over-estimates the waiting time about `r -round(error, 2)` minutes.

---

## Practice: Find Regression Line {.unnumbered}

::: {.iframecontainer}
`r knitr::include_url('https://www.myopenmath.com/embedq2.php?id=902994&seed=2020&showansafter')`
:::

---

## Residual Plot

- A residual is an observable estimate of the unobservable statistical error.
  ::: {.center}
  ![A image of positive and negative residuals](Figures/residual.png){width="30%"}  
  **Positive and Negative Residuals**
  :::

- **Residual plots**, a scatterplot of the $(x, \text{residual})$ can be used if a linear model is appropriate. A random pattern (or no obvious pattern) indicates a good fit of a linear model.  [See Assessing the Fit of a Line (2 of 4) in Concepts in Statistics for examples.](https://courses.lumenlearning.com/wmopen-concepts-statistics/chapter/assessing-the-fit-of-a-line-2-of-4/)

::: {.footmark}
Image source: [Figure 5.14 in Introduction to Statistics and Data Analysis](https://www.cengage.com/c/introduction-to-statistics-and-data-analysis-6e-peck/9781337793612PF/?searchIsbn=9780357686522)
:::

---

## Standard Error

- The **residual standard error** (or **standard error of the regression**), calculated by the Excel function `STEYX()`,  is 
  $$s_e=\sqrt{\dfrac{SS_{res}}{n-2}},$$
  where $SS_{res}=\sum (y-\hat{y})^2$ is the **sum of square residuals**.

- The standard error is a typical (average) amount that an observation deviates from the least-squares line.

- The smaller $s_e$ is, the more accurate the prediction is.

---

## Coefficient of Determination

**Coefficient of determination** measures the proportion of variability in the response variable $y$ that can be attributed to the linear regression line (a nice explanation can be found in [Explaining The Variance of a Regression Model](https://stats.stackexchange.com/q/203675)).
  
- The total variance of $y$ is the sum of square deviations $SS_{tot}=\sum(y-\bar{y})^2=(n-1)s_y^2$.

- The total variance of predicted $y$ is $SS_{reg}=\sum(\hat{y}-\bar{y})^2=(n-1)r^2s_y^2$.

- The **coefficient of determination** is $$\dfrac{SS_{reg}}{SS_{tot}}=\dfrac{(n-1)r^2s_y^2}{(n-1)s_y^2}=r^2.$$

::: {.footmark}
A visualization by Magnusson, K. can be found at <https://rpsychologist.com/correlation/>.
:::

---

## Remarks on Coefficient of Determination

- The $r$ in the coefficient of determination is the correlation coefficient. Equivalently, $r=\pm\sqrt{r^2}$.

- The smaller the standard error, the larger the coefficient of determination: $r^2=1-\dfrac{SS_{res}}{SS_{tot}}=1-\dfrac{(n-2)s_e^2}{SS_{tot}}$.

- $n−2$ is the degrees of freedom. We lose two degrees of freedom because both the slope and the $y$-intercept are estimated.

---

## Example: $s_e$ and $r^2$

::::: {.row}
::: {style="63%;"}
Find the standard error and coefficient of determination for the data of midterm1 and final.

**Solution:**
```{r, lm_grade, include=FALSE}
load("Data-Frames-SUNY-Concepts-in-Statistics/gradebook.rdata")
gradebook <- rename(data)
GradeBook <- gradebook[20:30,c("Midterm1", "Final")]
cor_GradeBook=round(cor(GradeBook$Midterm1, GradeBook$Final),digits=3)
model=lm(formula = Final ~ ., data=GradeBook)
```

In Excel, the function `STEYX()` can be used to obtain the residual standard error. In this example, $s_e\approx `r round(sqrt(deviance(model)/df.residual(model)),3)`$.

The correlation coefficient is $`r cor_GradeBook`$.

The coefficient determination is $r^2=`r cor_GradeBook`^2\approx `r round(cor_GradeBook^2,3)`$.
:::

:::: {style="35%;"}
::: {.center}
```{r echo=FALSE, result='asis'}
kable(GradeBook,
    format = "html",
    booktabs = TRUE,
    align = "c",
   row.names = FALSE
  ) %>%
  kable_styling(full_width = F, bootstrap_options = c("striped", "hover", "condensed"), font = 24) %>%
  column_spec(1:2, width="8em")
```
:::
::::
:::::

---

## Practice: Analyzing Linear Relationship {.unnumbered}

::: {.iframecontainer}
`r knitr::include_url('https://www.myopenmath.com/embedq2.php?id=905035&seed=2020&showansafter')`
:::

---

## References {.unnumbered}

- Magnusson, K. (2023). Interpreting Correlations: An interactive visualization (Version 0.7.1) [Web App]. R Psychologist. <https://rpsychologist.com/correlation/>.

- PennState STAT414 Lesson 18: The Correlation Coefficient <https://online.stat.psu.edu/stat414/lesson/18>.

- Wiki Correlation <https://en.wikipedia.org/wiki/Correlation>.

- Wiki Covariance <https://en.wikipedia.org/wiki/Covariance#Discrete_random_variables>

- Joseph Lee Rodgers; W. Alan Nicewander. Thirteen Ways to Look at the Correlation Coefficient <https://www.stat.berkeley.edu/~rabbee/correlation.pdf>.

- Matthew Gunn (<https://stats.stackexchange.com/users/97925/matthew-gunn>), How can the regression error term ever be correlated with the explanatory variables?

---

<!--# class="middle center" -->

::: {.part}
Lab Instruction in Excel
:::

---

## Scatter Plots and Correlation Coefficient

- To create a scatter plot, first select the data sets, and then look for `Insert Scatter(X, Y)` in the menu `Insert`-> `Charts`.

- The correlation coefficient $r$ can be calculated by the Excel function `correl()`.

---

## Slope, Intercept, $r^2$ and $s_e$

- The slope of a linear regression can be calculated by the Excel function `SLOPE()`.

- The $y$-intercept of a linear regression can be calculated by the Excel function `INTERCEPT()`.

- The coefficient of determination $r^2$ can be calculated by first finding $r$, then applying the formula `r^2`.

- The standard error $s_e$ of the regression (residual standard error) can be calculated by the Excel function `STEYX()`.

<!-- --- -->

<!-- ## Lab Practice: Annual Maximum Wind Speed {.unnumbered} -->

<!-- The annual maximum wind speed (in meters per second) in Hong Kong for each year in a 45-year period were in the linked file [HK Max Wind Speed](HKWindSpeed.xlsx). -->

<!-- 1. Use the annual maximum wind speed data to construct a histogram. -->

<!-- 2. Describe pattern (shape, center, spread, outliers) of the histogram. -->
